from transformers import DataCollatorForSeq2Seq
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import numpy as np
from datasets import load_from_disk
import evaluate
# from accelerate import Accelerator
from tqdm.auto import tqdm
import torch
from torch.utils.data import DataLoader
from transformers import get_scheduler
import uuid
from peft import LoraConfig, TaskType, get_peft_model
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed
import copy
import torch.nn.functional as F
import json 

# Useful pages: 
# https://www.youtube.com/watch?time_continue=3938&v=Pb_RGAl75VE&embeds_referring_euri=https%3A%2F%2Fwww.google.com%2F&source_ve_path=Mjg2NjMsMjg2NjY&feature=emb_logo
# https://github.com/Maykeye/BTLM-peft-test-4bit/blob/main/test-btlm.ipynb
# https://github.com/tcapelle/llm_recipes/blob/main/nbs/Alpaca_finetunning_with_WandB.ipynb

# accelerator = Accelerator()

show_examples = True
batch_size = 2

def load_some_data(amount, tokenized_datasets, data_collator, dataset = "train"):
    if dataset == "train":
        train_dataloader = DataLoader(
            tokenized_datasets["train"].shuffle(seed = uuid.uuid4().int % 2**32).select(range(amount)),
            shuffle=True,
            batch_size=batch_size,
            collate_fn=data_collator
        )
        # train_dataloader = accelerator.prepare( # Prepare with accelerator
        #     train_dataloader
        # )
        return train_dataloader

    elif dataset == "validation":
        eval_dataloader = DataLoader(
            tokenized_datasets["validation"].shuffle(seed = uuid.uuid4().int % 2**32).select(range(amount)),
            batch_size=20,
            collate_fn=data_collator
        )
        # eval_dataloader = accelerator.prepare( # Prepare with accelerator
        #     eval_dataloader
        # )
        return eval_dataloader
        
def main():
    # Define special tokens to indicate instructions
    B_INST, E_INST = "[INST]", "[/INST]"
    B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"

    def preprocess_function(examples):
        # Define prompts. Example:
        # [INST]<<SYS>>
        # Summarize the following conversation.
        # <</SYS>>
        # 
        # Hello! Alex here. Are you coming to the party?[/INST]
        # In this conversation, Alex asks if he's coming to the party.
        instructions = f"{B_INST}{B_SYS}Summarize the following conversation.{E_SYS}"
        prompts = [instructions + ex + E_INST + "\nIn this conversation, " for ex, sum in zip(examples['dialogue'], examples['summary'])]
        inputs = [prompt + sum + tokenizer.eos_token for prompt, sum in zip(prompts, examples['summary'])]
        targets = copy.deepcopy(inputs)
        model_inputs = tokenizer(inputs, text_target = targets, padding='max_length')

        # Set tokens to -100 if they are not generated by the model.
        # For example, consider the sentence "Hello, how are you? I am fine, thank you", encoded as:
        # [1, 15043, 29892, 920, 526, 366, 29973, 306, 626, 2691, 29892, 6452, 366, 2]
        # To only keep the tokens from "I am fine, thank you," set all previous tokens to -100:
        # [-100, -100, -100, -100, -100, -100, -100, 306, 626, 2691, 29892, 6452, 366, 2]

        # Tokens corresponding to "In this conversation,"
        split_tokens = torch.tensor([29914, 25580, 29962, 13, 797, 445, 14983, 29892])
        for i in range(len(model_inputs['labels'])):
            counter = 0
            for j, x in enumerate(model_inputs['labels'][i]):
                if counter == len(split_tokens):
                    break
                if x == split_tokens[counter]:
                    counter += 1
                else:
                    counter = 0
                model_inputs['labels'][i][j] = -100

        return model_inputs
    
    path = "/gpfs/projects/bsc03/bsc03268/tasks/meta-llama.Llama-2-7b-chat-hf-samsum"
    dataset = load_from_disk(path + "/dataset")
    tokenizer = AutoTokenizer.from_pretrained(path + "/tokenizer")
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=path + "/model")
    metric = evaluate.load('./metrics/rouge', experiment_id = uuid.uuid4().int % 2**32)
    device = torch.device('cuda')
    model = AutoModelForCausalLM.from_pretrained(path + "/model").to(device)

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    # Decoder-based models require "left" as padding-side
    # Otherwise, the model would generate text after a big amount of padding tokens
    # The model is not trained to manage this
    tokenizer.padding_side = "left"

    # Tokenize the whole dataset
    tokenized_datasets = dataset.map(
        preprocess_function,
        batched=True,
        batch_size=batch_size,
        remove_columns=dataset["train"].column_names,
    )
    tokenized_datasets.set_format("torch")

    # Create dataloaders we can iterate on
    train_dataloader = load_some_data(10, tokenized_datasets, data_collator, dataset = "train")
    # eval_dataloader = load_some_data(60, tokenized_datasets, data_collator, dataset = "validation")
    eval_dataloader = copy.deepcopy(train_dataloader) # TODO REMOVE THIS LINE

    ########## LORA ##########
    lora_config = LoraConfig(
        r=8,
        target_modules=["q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"],
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    model = get_peft_model(model, lora_config)
    ########## LORA ########## 

    num_train_epochs = 40
    num_training_steps = num_train_epochs * len(train_dataloader)

    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps,
    )

    progress_bar = tqdm(range(len(train_dataloader) * num_train_epochs))#, disable=not accelerator.is_local_main_process)
    losses = []
    metrics = []

    # Start training
    for epoch in range(num_train_epochs):
        print(f"\nEPOCH:{epoch}\n", flush=True)
        
        ########## TRAINING LOOP ##########
        model.train()
        for batch in train_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)
        ########## TRAINING LOOP ##########
 
        print(loss.item(), flush=True)
        losses.append(loss.item())

        ########## EVALUATION ##########
        model.eval()
        for batch in eval_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}

            # Remove the tokens corresponding to the response
            # Place these tokens at the end of the row, with padding tokens on the left
            new = torch.where(batch['labels'] == -100, batch['input_ids'], 2)
            for i in range(len(new)):
                idxs = torch.where(new[i] != 2)[0]
                new[i][-len(idxs):] = new[i][idxs]
                new[i][:-len(idxs)] = 2
            min = torch.sum(new == 2, dim = -1).min()
            new = new[:,min:] # Trim as many '2' tokens as possible

            sentences = {}
            sentences['input_ids'] = new
            sentences['attention_mask'] = torch.ones(new.size()).int().to('cuda')
            sentences['attention_mask'][new == 2] = 0

            # Generate text
            generated_tokens = model.generate(
                **sentences,
                max_new_tokens = 300,
                pad_token_id=tokenizer.pad_token_id
            )

            # Turn tokens into text and select the response
            decoded_sentences = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
            decoded_sentences = [x.split('In this conversation, ')[1] for x in decoded_sentences]
            decoded_labels = torch.where(batch['labels'] == -100, 2, batch['labels'])
            decoded_labels = tokenizer.batch_decode(decoded_labels, skip_special_tokens=True)

            if show_examples:
                print('---------------------', flush = True)
                print('Decoded sentences:')
                print(json.dumps(decoded_sentences, indent = 4), flush = True)
                print('Decoded labels:')
                print(json.dumps(decoded_labels, indent = 4), flush = True)

            # Compute metric
            metric.add_batch(predictions=decoded_sentences, references=decoded_labels)

        metrics.append(metric.compute())
        ########## EVALUATION ##########

    print(json.dumps(losses))
    print(json.dumps(metrics))

if __name__ == "__main__":
    main()