{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5szVnGRbT_-2"
      },
      "outputs": [],
      "source": [
        "pip install datasets > /dev/null\n",
        "pip install -U accelerate transformers > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpSs3ejCeLWz"
      },
      "outputs": [],
      "source": [
        "! pip install -U accelerate > /dev/null\n",
        "! pip install -U transformers  > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3z0VArEUE5P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzbvvdcyUIHj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "def load_model():\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/\")\n",
        "  return model\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True, padding=True)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='binary')\n",
        "    recall = recall_score(labels, preds, average='binary')\n",
        "    f1 = f1_score(labels, preds, average='binary')\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'matthews': mcc\n",
        "    }\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "dataset = load_dataset(\"glue\", \"cola\")\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)['validation']\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_eval_batch_size=100,\n",
        "    output_dir='./results',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "h6IJgIOkUK3g",
        "outputId": "543ee3f7-439a-4a0c-e41c-4c5c8f379aa9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.322219967842102,\n",
              " 'eval_accuracy': 0.3087248322147651,\n",
              " 'eval_precision': 0.0,\n",
              " 'eval_recall': 0.0,\n",
              " 'eval_f1': 0.0,\n",
              " 'eval_matthews': 0.0,\n",
              " 'eval_runtime': 1.2176,\n",
              " 'eval_samples_per_second': 856.592,\n",
              " 'eval_steps_per_second': 9.034}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = load_model()\n",
        "\n",
        "# Evaluate modified model and save result\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args = training_args,\n",
        "  compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "for x in model.state_dict().keys():\n",
        "  tensor = model.state_dict()[x]\n",
        "  if \".layer.\" in x and len(tensor.size()) == 2:\n",
        "    randomly_prune_blocks_by_area(tensor, area_percentage = 0.3, block_size = 32)\n",
        "\n",
        "trainer.evaluate(tokenized_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "IzpBzYETn31C",
        "outputId": "5dc0cdcf-07cc-4e97-e1e9-6d306dbcc1d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n",
            "[1 1 1 ... 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "predictions = trainer.predict(tokenized_dataset)\n",
        "\n",
        "pred_scores = np.argmax(predictions.predictions, axis=1)\n",
        "pred_labels = predictions.label_ids\n",
        "\n",
        "print(pred_scores)\n",
        "print(pred_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk9g_ggcnWDO"
      },
      "outputs": [],
      "source": [
        "print_weight_matrices(model.cpu(), ignore_zeros=True, visualization_mode='abs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cFCEazLJUPSi",
        "outputId": "07b3927a-1c1f-4f84-9017-50aa35e374c3"
      },
      "outputs": [],
      "source": [
        "## USING AREAS\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "fig, axs = plt.subplots(2, 3, figsize=(15, 9))\n",
        "values = np.arange(0, 0.4, 0.005)\n",
        "values_100 = 100 * values\n",
        "\n",
        "for block_size in [8, 16, 32, 64]:\n",
        "  eval_results = []\n",
        "  for area in values:\n",
        "    # Load model\n",
        "    model = load_model()\n",
        "\n",
        "    # Modify model\n",
        "    for x in model.state_dict().keys():\n",
        "      tensor = model.state_dict()[x]\n",
        "      if \".layer.\" in x and len(tensor.size()) == 2:\n",
        "        randomly_prune_blocks_by_area(tensor, area, block_size)\n",
        "\n",
        "    # Evaluate modified model and save result\n",
        "    trainer = Trainer(\n",
        "      model=model,\n",
        "      args = training_args,\n",
        "      compute_metrics=compute_metrics,\n",
        "    )\n",
        "    eval_results.append(trainer.evaluate(tokenized_dataset))\n",
        "\n",
        "  eval_loss = [x['eval_loss'] for x in eval_results]\n",
        "  eval_accuracy = [x['eval_accuracy'] for x in eval_results]\n",
        "  eval_precision = [x['eval_precision'] for x in eval_results]\n",
        "  eval_recall = [x['eval_recall'] for x in eval_results]\n",
        "  eval_f1 = [x['eval_f1'] for x in eval_results]\n",
        "  eval_matthews = [x['eval_matthews'] for x in eval_results]\n",
        "\n",
        "  axs[0, 0].plot(values_100, eval_loss, label = f\"Block size {block_size}\")\n",
        "  axs[0, 1].plot(values_100, eval_accuracy, label = f\"Block size {block_size}\")\n",
        "  axs[0, 2].plot(values_100, eval_precision, label = f\"Block size {block_size}\")\n",
        "  axs[1, 0].plot(values_100, eval_recall, label = f\"Block size {block_size}\")\n",
        "  axs[1, 1].plot(values_100, eval_f1, label = f\"Block size {block_size}\")\n",
        "  axs[1, 2].plot(values_100, eval_matthews, label = f\"Block size {block_size}\")\n",
        "\n",
        "axs[0, 0].set_title('Loss')\n",
        "axs[0, 0].set_xlabel('% of pruned area')\n",
        "axs[0, 0].set_ylabel('Loss')\n",
        "axs[0, 0].legend()\n",
        "\n",
        "axs[0, 1].set_title('Accuracy')\n",
        "axs[0, 1].set_xlabel('% of pruned area')\n",
        "axs[0, 1].set_ylabel('Accuracy')\n",
        "axs[0, 1].legend()\n",
        "\n",
        "axs[0, 2].set_title('Precision')\n",
        "axs[0, 2].set_xlabel('% of pruned area')\n",
        "axs[0, 2].set_ylabel('Precision')\n",
        "axs[0, 2].legend()\n",
        "\n",
        "axs[1, 0].set_title('Recall')\n",
        "axs[1, 0].set_xlabel('% of pruned area')\n",
        "axs[1, 0].set_ylabel('Recall')\n",
        "axs[1, 0].legend()\n",
        "\n",
        "axs[1, 1].set_title('F1 Score')\n",
        "axs[1, 1].set_xlabel('% of pruned area')\n",
        "axs[1, 1].set_ylabel('F1 Score')\n",
        "axs[1, 1].legend()\n",
        "\n",
        "axs[1, 2].set_title('Matthews Correlation')\n",
        "axs[1, 2].set_xlabel('% of pruned area')\n",
        "axs[1, 2].set_ylabel('Matthews Correlation')\n",
        "axs[1, 2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M00EosC6pnd8",
        "outputId": "2d125d0d-b59a-4620-f22b-d6c20b5dcd4e"
      },
      "outputs": [],
      "source": [
        "## USING AREAS\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "fig, axs = plt.subplots(2, 3, figsize=(15, 9))\n",
        "values = np.arange(0, 1, 0.05)\n",
        "values_100 = 100 * values\n",
        "\n",
        "for block_size in [8, 16, 32, 64]:\n",
        "  eval_results = []\n",
        "  for area in values:\n",
        "    # Load model\n",
        "    model = load_model()\n",
        "\n",
        "    # Modify model\n",
        "    for x in model.state_dict().keys():\n",
        "      tensor = model.state_dict()[x]\n",
        "      if \".layer.\" in x and len(tensor.size()) == 2:\n",
        "        randomly_prune_blocks_by_area(tensor, area, block_size)\n",
        "\n",
        "    # Evaluate modified model and save result\n",
        "    trainer = Trainer(\n",
        "      model=model,\n",
        "      args = training_args,\n",
        "      compute_metrics=compute_metrics,\n",
        "    )\n",
        "    eval_results.append(trainer.evaluate(tokenized_dataset))\n",
        "\n",
        "  eval_loss = [x['eval_loss'] for x in eval_results]\n",
        "  eval_accuracy = [x['eval_accuracy'] for x in eval_results]\n",
        "  eval_precision = [x['eval_precision'] for x in eval_results]\n",
        "  eval_recall = [x['eval_recall'] for x in eval_results]\n",
        "  eval_f1 = [x['eval_f1'] for x in eval_results]\n",
        "  eval_matthews = [x['eval_matthews'] for x in eval_results]\n",
        "\n",
        "  axs[0, 0].plot(values_100, eval_loss, label = f\"Block size {block_size}\")\n",
        "  axs[0, 1].plot(values_100, eval_accuracy, label = f\"Block size {block_size}\")\n",
        "  axs[0, 2].plot(values_100, eval_precision, label = f\"Block size {block_size}\")\n",
        "  axs[1, 0].plot(values_100, eval_recall, label = f\"Block size {block_size}\")\n",
        "  axs[1, 1].plot(values_100, eval_f1, label = f\"Block size {block_size}\")\n",
        "  axs[1, 2].plot(values_100, eval_matthews, label = f\"Block size {block_size}\")\n",
        "\n",
        "axs[0, 0].set_title('Loss')\n",
        "axs[0, 0].set_xlabel('% of pruned area')\n",
        "axs[0, 0].set_ylabel('Loss')\n",
        "axs[0, 0].legend()\n",
        "\n",
        "axs[0, 1].set_title('Accuracy')\n",
        "axs[0, 1].set_xlabel('% of pruned area')\n",
        "axs[0, 1].set_ylabel('Accuracy')\n",
        "axs[0, 1].legend()\n",
        "\n",
        "axs[0, 2].set_title('Precision')\n",
        "axs[0, 2].set_xlabel('% of pruned area')\n",
        "axs[0, 2].set_ylabel('Precision')\n",
        "axs[0, 2].legend()\n",
        "\n",
        "axs[1, 0].set_title('Recall')\n",
        "axs[1, 0].set_xlabel('% of pruned area')\n",
        "axs[1, 0].set_ylabel('Recall')\n",
        "axs[1, 0].legend()\n",
        "\n",
        "axs[1, 1].set_title('F1 Score')\n",
        "axs[1, 1].set_xlabel('% of pruned area')\n",
        "axs[1, 1].set_ylabel('F1 Score')\n",
        "axs[1, 1].legend()\n",
        "\n",
        "axs[1, 2].set_title('Matthews Correlation')\n",
        "axs[1, 2].set_xlabel('% of pruned area')\n",
        "axs[1, 2].set_ylabel('Matthews Correlation')\n",
        "axs[1, 2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F-Me9jsNq33y",
        "outputId": "f65d803a-35ac-41dc-bb9d-fe83b3704c30"
      },
      "outputs": [],
      "source": [
        "## USING AREAS\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "fig, axs = plt.subplots(2, 3, figsize=(15, 9))\n",
        "values = np.arange(0, 0.1, 0.001)\n",
        "values_100 = 100 * values\n",
        "\n",
        "for block_size in [8, 16, 32, 64]:\n",
        "  eval_results = []\n",
        "  for area in values:\n",
        "    # Load model\n",
        "    model = load_model()\n",
        "\n",
        "    # Modify model\n",
        "    for x in model.state_dict().keys():\n",
        "      tensor = model.state_dict()[x]\n",
        "      if \".layer.\" in x and len(tensor.size()) == 2:\n",
        "        randomly_prune_blocks_by_area(tensor, area, block_size)\n",
        "\n",
        "    # Evaluate modified model and save result\n",
        "    trainer = Trainer(\n",
        "      model=model,\n",
        "      args = training_args,\n",
        "      compute_metrics=compute_metrics,\n",
        "    )\n",
        "    eval_results.append(trainer.evaluate(tokenized_dataset))\n",
        "\n",
        "  eval_loss = [x['eval_loss'] for x in eval_results]\n",
        "  eval_accuracy = [x['eval_accuracy'] for x in eval_results]\n",
        "  eval_precision = [x['eval_precision'] for x in eval_results]\n",
        "  eval_recall = [x['eval_recall'] for x in eval_results]\n",
        "  eval_f1 = [x['eval_f1'] for x in eval_results]\n",
        "  eval_matthews = [x['eval_matthews'] for x in eval_results]\n",
        "\n",
        "  axs[0, 0].plot(values_100, eval_loss, label = f\"Block size {block_size}\")\n",
        "  axs[0, 1].plot(values_100, eval_accuracy, label = f\"Block size {block_size}\")\n",
        "  axs[0, 2].plot(values_100, eval_precision, label = f\"Block size {block_size}\")\n",
        "  axs[1, 0].plot(values_100, eval_recall, label = f\"Block size {block_size}\")\n",
        "  axs[1, 1].plot(values_100, eval_f1, label = f\"Block size {block_size}\")\n",
        "  axs[1, 2].plot(values_100, eval_matthews, label = f\"Block size {block_size}\")\n",
        "\n",
        "axs[0, 0].set_title('Loss')\n",
        "axs[0, 0].set_xlabel('% of pruned area')\n",
        "axs[0, 0].set_ylabel('Loss')\n",
        "axs[0, 0].legend()\n",
        "\n",
        "axs[0, 1].set_title('Accuracy')\n",
        "axs[0, 1].set_xlabel('% of pruned area')\n",
        "axs[0, 1].set_ylabel('Accuracy')\n",
        "axs[0, 1].legend()\n",
        "\n",
        "axs[0, 2].set_title('Precision')\n",
        "axs[0, 2].set_xlabel('% of pruned area')\n",
        "axs[0, 2].set_ylabel('Precision')\n",
        "axs[0, 2].legend()\n",
        "\n",
        "axs[1, 0].set_title('Recall')\n",
        "axs[1, 0].set_xlabel('% of pruned area')\n",
        "axs[1, 0].set_ylabel('Recall')\n",
        "axs[1, 0].legend()\n",
        "\n",
        "axs[1, 1].set_title('F1 Score')\n",
        "axs[1, 1].set_xlabel('% of pruned area')\n",
        "axs[1, 1].set_ylabel('F1 Score')\n",
        "axs[1, 1].legend()\n",
        "\n",
        "axs[1, 2].set_title('Matthews Correlation')\n",
        "axs[1, 2].set_xlabel('% of pruned area')\n",
        "axs[1, 2].set_ylabel('Matthews Correlation')\n",
        "axs[1, 2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m23\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(a)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "a = [1,23,3,4]\n",
        "\n",
        "plt.plot(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L77a1ZbjUSQe"
      },
      "outputs": [],
      "source": [
        "print_weight_matrices(model.cpu(), ignore_zeros=True, visualization_mode='abs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMWx6PHeXINC",
        "outputId": "d8e1dd09-01c2-4ef4-8457-f4fad064f5c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.1758610010147095, 'eval_accuracy': 0.5445829338446788, 'eval_f1': 0.5383867832847424, 'eval_precision': 0.8993506493506493, 'eval_recall': 0.3841886269070735, 'eval_matthews': 0.2915689932969142, 'eval_runtime': 1.0908, 'eval_samples_per_second': 956.205, 'eval_steps_per_second': 10.085}\n"
          ]
        }
      ],
      "source": [
        "print(eval_results[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
